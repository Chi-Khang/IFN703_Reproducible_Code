{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9571f33d-1bd7-4a80-9bba-edec5888d684",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddb98dcc-d4db-45af-b588-5b404de20162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yolov5.utils\n",
    "from yolov5.utils.dataloaders import LoadImages\n",
    "from yolov5.utils.general import non_max_suppression, xyxy2xywh\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ea21a82-0a4b-43aa-855f-57f4d3968626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/khang/Documents/IFN703'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "current_path = os.getcwd()\n",
    "current_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f13dbcca-f2f5-43ab-8ca0-a58bccae9826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce89388-1ba3-4e35-be7d-e359b05e7c6d",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a758268d-c8eb-4efb-b97b-c31aaf1944f3",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8e0f15b-aa29-4fa0-985d-2c9b75f39e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters (Change for making different models for parameters tuning)\n",
    "history_size = 10\n",
    "action_option = 9\n",
    "max_steps = 10\n",
    "experience_sample_size = 20\n",
    "max_experience_size = 1000\n",
    "gamma = 0.9\n",
    "epsilon_change_steps = 10\n",
    "loss_arr = []\n",
    "IOU_STOP = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b71f74d-23c4-4238-9236-45856d2b6c33",
   "metadata": {},
   "source": [
    "## Extract Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5d08c08-f382-4a76-a8fb-1522dbdd115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from image and history\n",
    "def extract_feature(image, history, vgg16):\n",
    "    history_feature = np.zeros(action_option * history_size)\n",
    "    for i in range(history_size):\n",
    "        if history[i] != -1:\n",
    "            history_feature[i * action_option + history[i]] = 1\n",
    "\n",
    "    feature_extractor = Model(inputs=vgg16.input, outputs=vgg16.layers[20].output)\n",
    "\n",
    "    image_reshape = [(cv2.resize(image, (224, 224))).reshape(1, 224, 224, 3)]\n",
    "    image_feature = feature_extractor(image_reshape)[0]\n",
    "    image_feature = np.ndarray.flatten(image_feature.numpy())\n",
    "    feature = np.concatenate((image_feature, history_feature))\n",
    "\n",
    "    return np.array([feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af63f419-6a09-4cd9-b1e1-bf4a1ec0fdc8",
   "metadata": {},
   "source": [
    "## Computing Values for Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dc7e46f-eca1-4aa0-b323-5f69d004339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Value computation\n",
    "def compute_q(feature, deep_q_model):\n",
    "    output = deep_q_model.predict(feature, verbose=0)\n",
    "    return np.ndarray.flatten(output)\n",
    "\n",
    "# Compute IoU (Intersection over Union) between the predicted mask and ground truth\n",
    "def compute_iou(mask, ground_truth):\n",
    "    dx = min(mask[2], ground_truth[2]) - max(mask[0], ground_truth[0])\n",
    "    dy = min(mask[3], ground_truth[3]) - max(mask[1], ground_truth[1])\n",
    "\n",
    "    if dx >= 0 and dy >= 0:\n",
    "        inter_area = dx * dy\n",
    "    else:\n",
    "        inter_area = 0\n",
    "\n",
    "    mask_area = (mask[2] - mask[0]) * (mask[3] - mask[1])\n",
    "    ground_truth_area = (ground_truth[2] - ground_truth[0]) * (ground_truth[3] - ground_truth[1])\n",
    "\n",
    "    return inter_area / (mask_area + ground_truth_area - inter_area)\n",
    "\n",
    "# Compute reward based on the IoU change\n",
    "def compute_reward(action, ground_truth, current_mask):\n",
    "    new_mask = compute_mask(action, current_mask)\n",
    "    iou_new = compute_iou(new_mask, ground_truth)\n",
    "    iou_current = compute_iou(current_mask, ground_truth)\n",
    "\n",
    "    return 1 if iou_current < iou_new else -1\n",
    "\n",
    "# Compute reward at the end of an episode\n",
    "def compute_end_reward(current_mask, ground_truth):\n",
    "    iou = compute_iou(current_mask, ground_truth)\n",
    "    print(\"CURRENT IOU =\", iou)\n",
    "    return 3 if iou > IOU_STOP else -3\n",
    "\n",
    "# Update target values for experience replay\n",
    "def compute_target(reward, new_feature, model):\n",
    "    return reward + gamma * np.amax(compute_q(new_feature, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98a338c-de46-4acf-b7b3-49b89b50b5f4",
   "metadata": {},
   "source": [
    "## Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79e0cd51-eaf6-4c32-ad9b-cb5e9c44db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute new mask based on the action\n",
    "def compute_mask(action, current_mask):\n",
    "    image_rate = 0.1\n",
    "    delta_width = image_rate * (current_mask[2] - current_mask[0])\n",
    "    delta_height = image_rate * (current_mask[3] - current_mask[1])\n",
    "\n",
    "    dx1, dy1, dx2, dy2 = 0, 0, 0, 0\n",
    "\n",
    "    # Apply the action to modify the bounding box (mask)\n",
    "    if action == 0:\n",
    "        dx1, dx2 = delta_width, delta_width\n",
    "    elif action == 1:\n",
    "        dx1, dx2 = -delta_width, -delta_width\n",
    "    elif action == 2:\n",
    "        dy1, dy2 = delta_height, delta_height\n",
    "    elif action == 3:\n",
    "        dy1, dy2 = -delta_height, -delta_height\n",
    "    elif action == 4:\n",
    "        dx1, dx2, dy1, dy2 = -delta_width, delta_width, -delta_height, delta_height\n",
    "    elif action == 5:\n",
    "        dx1, dx2, dy1, dy2 = delta_width, -delta_width, delta_height, -delta_height\n",
    "    elif action == 6:\n",
    "        dy1, dy2 = delta_height, -delta_height\n",
    "    elif action == 7:\n",
    "        dx1, dx2 = delta_width, -delta_width\n",
    "\n",
    "    new_mask_tmp = np.array([current_mask[0] + dx1, current_mask[1] + dy1,\n",
    "                             current_mask[2] + dx2, current_mask[3] + dy2])\n",
    "    new_mask = np.array([\n",
    "        min(new_mask_tmp[0], new_mask_tmp[2]),\n",
    "        min(new_mask_tmp[1], new_mask_tmp[3]),\n",
    "        max(new_mask_tmp[0], new_mask_tmp[2]),\n",
    "        max(new_mask_tmp[1], new_mask_tmp[3])\n",
    "    ])\n",
    "\n",
    "    return new_mask\n",
    "\n",
    "# Action selection using epsilon-greedy\n",
    "def select_action(feature, ground_truth_box, step, q_value, epsilon, current_mask):\n",
    "    if step == max_steps:\n",
    "        action = 8 #select trigger if agent surpassed maximum number of steps\n",
    "\n",
    "    else:\n",
    "        if random.random() > epsilon:\n",
    "            action = np.argmax(q_value)\n",
    "        else:\n",
    "            end_reward = compute_end_reward(current_mask, ground_truth_box)\n",
    "            if end_reward > 0 and step != 0:\n",
    "                action = 8\n",
    "            else:\n",
    "                rewards = []\n",
    "                for i in range(action_option - 1):\n",
    "                    reward = compute_reward(i, ground_truth_box, current_mask)\n",
    "                    rewards.append(reward)\n",
    "                rewards = np.asarray(rewards)\n",
    "                positive_reward_index = np.where(rewards >= 0)[0]\n",
    "\n",
    "                if len(positive_reward_index) == 0:\n",
    "                    positive_reward_index = np.asarray(range(9))\n",
    "\n",
    "                action = np.random.choice(positive_reward_index)\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "# Execute action and update mask, history, and reward\n",
    "def execute_action(action, history, ground_truth_box, current_mask):\n",
    "    if action == 8:\n",
    "        new_mask = current_mask\n",
    "        reward = compute_end_reward(current_mask, ground_truth_box)\n",
    "        end = True\n",
    "    else:\n",
    "        new_mask = compute_mask(action, current_mask)\n",
    "        reward = compute_reward(action, ground_truth_box, current_mask)\n",
    "        history = history[1:]\n",
    "        history.append(action)\n",
    "        end = False\n",
    "\n",
    "    return new_mask, reward, end, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304aabd8-a5f7-4734-82cb-a917ecfb1c6e",
   "metadata": {},
   "source": [
    "## Crop image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73a52e17-676f-4138-b135-dcd2b8087abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop the image based on the mask\n",
    "def crop_image(image, new_mask):\n",
    "    height, width, channel = np.shape(image)\n",
    "    new_mask = np.asarray(new_mask).astype(\"int\")\n",
    "    new_mask[0] = max(new_mask[0], 0)\n",
    "    new_mask[1] = max(new_mask[1], 0)\n",
    "    new_mask[2] = min(new_mask[2], width)\n",
    "    new_mask[3] = min(new_mask[3], height)\n",
    "    cropped_image = image[new_mask[1]:new_mask[3], new_mask[0]:new_mask[2]]\n",
    "    new_height, new_width, new_channel = np.shape(cropped_image)\n",
    "\n",
    "    if new_height == 0 or new_width == 0:\n",
    "        cropped_image = np.zeros((224, 224, 3))\n",
    "    else:\n",
    "        cv2.resize(cropped_image, (224, 224))\n",
    "\n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4348b889-c03d-4699-8ae6-dd6b5e71d371",
   "metadata": {},
   "source": [
    "## Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eae9c0fc-3efa-4b76-9f90-9ef733c4dcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience replay to train the model\n",
    "def experience_replay(deep_q_model, experience):\n",
    "    sample = random.choices(experience, k=experience_sample_size)\n",
    "\n",
    "    targets = np.zeros((experience_sample_size, action_option))\n",
    "\n",
    "    for i in range(experience_sample_size):\n",
    "        feature, action, new_feature, reward, end = sample[i]\n",
    "        target = reward\n",
    "\n",
    "        if not end:\n",
    "            target = compute_target(reward, new_feature, deep_q_model)\n",
    "\n",
    "        targets[i, :] = compute_q(feature, deep_q_model)\n",
    "        targets[i][action] = target\n",
    "\n",
    "    x = np.concatenate([each[0] for each in sample])\n",
    "\n",
    "    global loss_arr\n",
    "    loss = deep_q_model.train_on_batch(x, targets)\n",
    "    loss_arr.append(loss)\n",
    "    if len(loss_arr) == 100:\n",
    "        print(\"loss %s\" % str(sum(loss_arr) / len(loss_arr)))\n",
    "        loss_arr = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091affdf-80d2-4692-9729-0321473e715d",
   "metadata": {},
   "source": [
    "## Read Images and Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a801f43e-9c4d-4c8d-a358-5e424bb34aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_index(basepath, test=True):\n",
    "    \"\"\"\n",
    "    Reading the names of images from the directory structure.\n",
    "    \"\"\"\n",
    "    index_list = []\n",
    "    image_folder_path = os.path.join(basepath, \"images\")\n",
    "    for image_file in os.listdir(image_folder_path):\n",
    "        if image_file.endswith(\".jpg\"):\n",
    "            index_list.append(image_file.split(\".\")[0])\n",
    "    return index_list\n",
    "\n",
    "def read_image(basepath):\n",
    "    \"\"\"\n",
    "    Loading images using their name from the images folder.\n",
    "    \"\"\"\n",
    "    image_list = []\n",
    "    # Directories\n",
    "    img_dir = Path(basepath + '/images')\n",
    "\n",
    "    # List all image files in the directory\n",
    "    supported_formats = ['.jpg', '.jpeg', '.png', '.bmp']\n",
    "    image_paths = [p for p in img_dir.glob('*') if p.suffix.lower() in supported_formats]\n",
    "\n",
    "    print(f\"Found {len(image_paths)} images in {img_dir}\")\n",
    "\n",
    "    for img_path in image_paths:\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is not None:\n",
    "            image_list.append(img)\n",
    "        else:\n",
    "            print(f\"Warning: {img_path} not found or invalid.\")\n",
    "    return image_list\n",
    "\n",
    "def convert_yolo_label_to_coords(box, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Convert YOLO format (relative coordinates) to pixel coordinates.\n",
    "    YOLO format is (x_center, y_center, width, height), all normalized.\n",
    "    \"\"\"\n",
    "    x_center, y_center, width, height = box\n",
    "    x_center *= img_width\n",
    "    y_center *= img_height\n",
    "    width *= img_width\n",
    "    height *= img_height\n",
    "    xmin = int(x_center - width / 2)\n",
    "    ymin = int(y_center - height / 2)\n",
    "    xmax = int(x_center + width / 2)\n",
    "    ymax = int(y_center + height / 2)\n",
    "    return [xmin, ymin, xmax, ymax]\n",
    "\n",
    "def load_annotation(basepath, filtered_class):\n",
    "    \"\"\"\n",
    "    Loading bounding boxes from TXT annotations in labels folder.\n",
    "    Only extracts images with the \"car\" label.\n",
    "    \"\"\"\n",
    "    bounding_box_list = []\n",
    "    annotation_folder_path = os.path.join(basepath, \"labels\")\n",
    "\n",
    "    # Directories\n",
    "    img_dir = Path(basepath + '/images')\n",
    "    lbl_dir = Path(basepath + '/labels')\n",
    "\n",
    "    # List all image files in the directory\n",
    "    supported_formats = ['.jpg', '.jpeg', '.png', '.bmp']\n",
    "    image_paths = [p for p in img_dir.glob('*') if p.suffix.lower() in supported_formats]\n",
    "\n",
    "    for img_path in image_paths:\n",
    "        label_path = lbl_dir / (img_path.stem + '.txt')\n",
    "\n",
    "        #try:\n",
    "        # Read image to get dimensions\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img_height, img_width = img.shape[:2]\n",
    "\n",
    "        # Read ground truth labels from TXT\n",
    "        with open(label_path, 'r') as f:\n",
    "            ground_truth = [list(map(float, line.strip().split())) for line in f.readlines()]\n",
    "\n",
    "        car_boxes = []\n",
    "        # Filter for \"car\" label (label index 5 in the provided names list)\n",
    "        for box in ground_truth:\n",
    "            if len(box) == 5 and int(box[0]) == filtered_class: \n",
    "                car_boxes.append(convert_yolo_label_to_coords(box[1:], img_width, img_height))\n",
    "        #if car_boxes:\n",
    "        bounding_box_list.append(car_boxes)\n",
    "\n",
    "        #except Exception as e:\n",
    "        #    print(f\"Error reading annotation for {img_path}: {e}\")\n",
    "        #    bounding_box_list.append([])  # Empty box list if failed to read annotation\n",
    "\n",
    "    return bounding_box_list\n",
    "\n",
    "def load_data_fil(dataset_path, test=False, filtered_class=5):\n",
    "    \"\"\"\n",
    "    Loading dataset images and their corresponding bounding boxes.\n",
    "    \"\"\"\n",
    "    image_index = read_image_index(dataset_path, test)\n",
    "    image_list = read_image(dataset_path)\n",
    "    bounding_box_list = load_annotation(dataset_path, filtered_class)\n",
    "\n",
    "    # Optionally save to .npy files if needed\n",
    "    if test:\n",
    "        np.save(\"val_images.npy\", image_list)\n",
    "        np.save(\"val_boxes.npy\", bounding_box_list)\n",
    "\n",
    "    print(bounding_box_list[:5])\n",
    "    print(\"DONE LOADING\")\n",
    "\n",
    "    return image_list, bounding_box_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b4e86a-360c-46a9-8bec-9a8bb172f921",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62593341-5063-453c-9515-6deba680213f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 362 images in valid/images\n",
      "[[[96, 284, 540, 538]], [[33, 225, 509, 605]], [], [[146, 234, 616, 598]], [[101, 3, 555, 486]]]\n",
      "DONE LOADING\n",
      "Selected 300 images and their corresponding bounding boxes.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "image_list, bounding_box_list = load_data_fil(\"valid\", filtered_class=5)  # Assuming this is the correct function name\n",
    "\n",
    "# Ensure image_list and bounding_box_list have the same length\n",
    "assert len(image_list) == len(bounding_box_list), \"Image list and bounding box list must have the same length.\"\n",
    "\n",
    "# Initialize lists for selected images and bounding boxes\n",
    "random_image_list = []\n",
    "random_bounding_box_list = []\n",
    "\n",
    "# Select 300 valid random images with non-empty bounding boxes\n",
    "while len(random_image_list) < 300:\n",
    "    random_index = random.randint(0, len(image_list) - 1)\n",
    "    if len(bounding_box_list[random_index]) > 0:  # Check if bounding box list is non-empty\n",
    "        random_image_list.append(image_list[random_index])\n",
    "        random_bounding_box_list.append(bounding_box_list[random_index])\n",
    "\n",
    "print(f\"Selected {len(random_image_list)} images and their corresponding bounding boxes.\")\n",
    "\n",
    "# Check for bounding box lists\n",
    "for box_lst in random_bounding_box_list:\n",
    "    if len(box_lst) == 0:\n",
    "        print(\"Found an empty bounding box list, which should not happen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3475f995-03d2-46c5-90c5-6c85c5c122ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 362 images in valid/images\n",
      "[[], [], [[133, 248, 499, 504]], [], []]\n",
      "DONE LOADING\n",
      "Selected 38 images and their corresponding bounding boxes.\n"
     ]
    }
   ],
   "source": [
    "# Load person data\n",
    "person_image_list, person_bounding_box_list = load_data_fil(\"valid\", filtered_class=8)  # Assuming this is the correct function name\n",
    "\n",
    "# Ensure image_list and bounding_box_list have the same length\n",
    "assert len(person_image_list) == len(person_bounding_box_list), \"Image list and bounding box list must have the same length.\"\n",
    "\n",
    "# Initialize lists for selected images and bounding boxes\n",
    "selected_person_image_list = []\n",
    "selected_person_bounding_box_list = []\n",
    "\n",
    "# Select 100 valid random images with non-empty bounding boxes\n",
    "for i in range(len(person_bounding_box_list )):\n",
    "    if len(person_bounding_box_list[i]) > 0 and len(selected_person_bounding_box_list) <= 100:  # Check if bounding box list is non-empty\n",
    "        selected_person_image_list.append(person_image_list[i])\n",
    "        selected_person_bounding_box_list.append(person_bounding_box_list[i])\n",
    "\n",
    "print(f\"Selected {len(selected_person_image_list)} images and their corresponding bounding boxes.\")\n",
    "\n",
    "# Check for bounding box lists\n",
    "for box_lst in selected_person_bounding_box_list:\n",
    "    if len(box_lst) == 0:\n",
    "        print(\"Found an empty bounding box list, which should not happen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5252ff2-9702-4f3f-a5ba-f7119343505b",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7b47d22-0dba-42ef-aa50-cd6373a1098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUBER_DELTA = 1.0\n",
    "def smoothL1(y_true, y_pred):\n",
    "    x = K.abs(y_true - y_pred)\n",
    "    x = tf.where(x < HUBER_DELTA, 0.5 * x ** 2, HUBER_DELTA * (x - 0.5 * HUBER_DELTA))\n",
    "    return K.sum(x)\n",
    "\n",
    "\n",
    "def create_q_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_shape=(4096 + action_option*history_size,), activation='relu'))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(9, activation='linear'))\n",
    "    model.compile(loss=smoothL1, optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_vgg16():\n",
    "    vgg16 = VGG16(weights='imagenet', include_top=True , pooling='max')\n",
    "    #vgg16.summary()\n",
    "    return vgg16\n",
    "\n",
    "\n",
    "# Change Epochs\n",
    "training_epoch = 10\n",
    "epsilon = 1\n",
    "#image_list, bounding_box_list = load_data(\"\",test=False)\n",
    "deep_q_model = create_q_model()\n",
    "vgg16 = create_vgg16()\n",
    "\n",
    "trained_model = train_deep_q(training_epoch, epsilon, random_image_list, random_bounding_box_list, deep_q_model, vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4128506-d13d-4553-8899-b00e0e9dcef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Person model\n",
    "trained_model_person = train_deep_q(training_epoch, selected_person_image_list, selected_person_bounding_box_list, epsilon, , deep_q_model, vgg16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3bee79-13e6-4107-a6c9-af1c3e507eb8",
   "metadata": {},
   "source": [
    "# Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267df052-552f-4a05-92db-2d0005532592",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_the_model = \"well_train_model_max_step_\" + str(max_steps) + \"_gamma_\" + str(gamma) \\\n",
    "    + \"_epochs_\" + str(training_epoch) + \"_trigger_threshold_\" + str(IOU_STOP) + \".h5\"\n",
    "trained_model.save(name_of_the_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d18808-856f-47f0-9dbc-ad8c5291b4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_the_model = \"person_well_train_model_max_step_\" + str(max_steps) + \"_gamma_\" + str(gamma) \\\n",
    "    + \"_epochs_\" + str(training_epoch) + \"_trigger_threshold_\" + str(IOU_STOP) + \".h5\"\n",
    "trained_model_person.save(name_of_the_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
