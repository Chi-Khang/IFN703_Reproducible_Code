{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e6da5c0-8239-4d56-923e-f5bffecf8fc0",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f9b71753-4970-4b6d-b2ee-eedec3f81e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yolov5.utils\n",
    "from yolov5.utils.dataloaders import LoadImages\n",
    "from yolov5.utils.general import non_max_suppression, xyxy2xywh\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76b43d1d-927b-49e9-b2b6-a61da2ad41af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/khang/Documents/IFN703'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "current_path = os.getcwd()\n",
    "current_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d9331a17-af3e-42f1-b2ac-bea5f96863b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d69a68c-ecc5-4063-a584-06832ee0e415",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5287a667-afc6-414d-858b-65904bab8f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUBER_DELTA = 1.0\n",
    "def smoothL1(y_true, y_pred):\n",
    "    x = K.abs(y_true - y_pred)\n",
    "    x = tf.where(x < HUBER_DELTA, 0.5 * x ** 2, HUBER_DELTA * (x - 0.5 * HUBER_DELTA))\n",
    "    return K.sum(x)\n",
    "\n",
    "\n",
    "def create_q_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_shape=(4096 + action_option*history_size,), activation='relu'))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(9, activation='linear'))\n",
    "    model.compile(loss=smoothL1, optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_vgg16():\n",
    "    vgg16 = VGG16(weights='imagenet', include_top=True , pooling='max')\n",
    "    #vgg16.summary()\n",
    "    return vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1f55dee1-9ec7-4ffd-b140-e6948f5db15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_index(basepath, test=True):\n",
    "    \"\"\"\n",
    "    Reading the names of images from the directory structure.\n",
    "    \"\"\"\n",
    "    index_list = []\n",
    "    image_folder_path = os.path.join(basepath, \"images\")\n",
    "    for image_file in os.listdir(image_folder_path):\n",
    "        if image_file.endswith(\".jpg\"):\n",
    "            index_list.append(image_file.split(\".\")[0])\n",
    "    return index_list\n",
    "\n",
    "def read_image(basepath):\n",
    "    \"\"\"\n",
    "    Loading images using their name from the images folder.\n",
    "    \"\"\"\n",
    "    image_list = []\n",
    "    # Directories\n",
    "    img_dir = Path(basepath + '/images')\n",
    "\n",
    "    # List all image files in the directory\n",
    "    supported_formats = ['.jpg', '.jpeg', '.png', '.bmp']\n",
    "    image_paths = [p for p in img_dir.glob('*') if p.suffix.lower() in supported_formats]\n",
    "\n",
    "    print(f\"Found {len(image_paths)} images in {img_dir}\")\n",
    "\n",
    "    for img_path in image_paths:\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is not None:\n",
    "            image_list.append(img)\n",
    "        else:\n",
    "            print(f\"Warning: {img_path} not found or invalid.\")\n",
    "    return image_list\n",
    "\n",
    "def convert_yolo_label_to_coords(box, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Convert YOLO format (relative coordinates) to pixel coordinates.\n",
    "    YOLO format is (x_center, y_center, width, height), all normalized.\n",
    "    \"\"\"\n",
    "    x_center, y_center, width, height = box\n",
    "    x_center *= img_width\n",
    "    y_center *= img_height\n",
    "    width *= img_width\n",
    "    height *= img_height\n",
    "    xmin = int(x_center - width / 2)\n",
    "    ymin = int(y_center - height / 2)\n",
    "    xmax = int(x_center + width / 2)\n",
    "    ymax = int(y_center + height / 2)\n",
    "    return [xmin, ymin, xmax, ymax]\n",
    "\n",
    "def load_annotation(basepath, filtered_class):\n",
    "    \"\"\"\n",
    "    Loading bounding boxes from TXT annotations in labels folder.\n",
    "    Only extracts images with the \"car\" label.\n",
    "    \"\"\"\n",
    "    bounding_box_list = []\n",
    "    annotation_folder_path = os.path.join(basepath, \"labels\")\n",
    "\n",
    "    # Directories\n",
    "    img_dir = Path(basepath + '/images')\n",
    "    lbl_dir = Path(basepath + '/labels')\n",
    "\n",
    "    # List all image files in the directory\n",
    "    supported_formats = ['.jpg', '.jpeg', '.png', '.bmp']\n",
    "    image_paths = [p for p in img_dir.glob('*') if p.suffix.lower() in supported_formats]\n",
    "\n",
    "    for img_path in image_paths:\n",
    "        label_path = lbl_dir / (img_path.stem + '.txt')\n",
    "\n",
    "        #try:\n",
    "        # Read image to get dimensions\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img_height, img_width = img.shape[:2]\n",
    "\n",
    "        # Read ground truth labels from TXT\n",
    "        with open(label_path, 'r') as f:\n",
    "            ground_truth = [list(map(float, line.strip().split())) for line in f.readlines()]\n",
    "\n",
    "        car_boxes = []\n",
    "        # Filter for \"car\" label (label index 5 in the provided names list)\n",
    "        for box in ground_truth:\n",
    "            if len(box) == 5 and int(box[0]) == filtered_class: \n",
    "                car_boxes.append(convert_yolo_label_to_coords(box[1:], img_width, img_height))\n",
    "        #if car_boxes:\n",
    "        bounding_box_list.append(car_boxes)\n",
    "\n",
    "        #except Exception as e:\n",
    "        #    print(f\"Error reading annotation for {img_path}: {e}\")\n",
    "        #    bounding_box_list.append([])  # Empty box list if failed to read annotation\n",
    "\n",
    "    return bounding_box_list\n",
    "\n",
    "def load_data_fil(dataset_path, test=False, filtered_class=5):\n",
    "    \"\"\"\n",
    "    Loading dataset images and their corresponding bounding boxes.\n",
    "    \"\"\"\n",
    "    image_index = read_image_index(dataset_path, test)\n",
    "    image_list = read_image(dataset_path)\n",
    "    bounding_box_list = load_annotation(dataset_path, filtered_class)\n",
    "\n",
    "    # Optionally save to .npy files if needed\n",
    "    if test:\n",
    "        np.save(\"val_images.npy\", image_list)\n",
    "        np.save(\"val_boxes.npy\", bounding_box_list)\n",
    "\n",
    "    print(bounding_box_list[:5])\n",
    "    print(\"DONE LOADING\")\n",
    "\n",
    "    return image_list, bounding_box_list\n",
    "\n",
    "# Compute IoU (Intersection over Union) between the predicted mask and ground truth\n",
    "def compute_iou(mask, ground_truth):\n",
    "    dx = min(mask[2], ground_truth[2]) - max(mask[0], ground_truth[0])\n",
    "    dy = min(mask[3], ground_truth[3]) - max(mask[1], ground_truth[1])\n",
    "\n",
    "    if dx >= 0 and dy >= 0:\n",
    "        inter_area = dx * dy\n",
    "    else:\n",
    "        inter_area = 0\n",
    "\n",
    "    mask_area = (mask[2] - mask[0]) * (mask[3] - mask[1])\n",
    "    ground_truth_area = (ground_truth[2] - ground_truth[0]) * (ground_truth[3] - ground_truth[1])\n",
    "\n",
    "    return inter_area / (mask_area + ground_truth_area - inter_area)\n",
    "\n",
    "# Function to extract features from image and history\n",
    "def extract_feature(image, history, vgg16):\n",
    "    history_feature = np.zeros(action_option * history_size)\n",
    "    for i in range(history_size):\n",
    "        if history[i] != -1:\n",
    "            history_feature[i * action_option + history[i]] = 1\n",
    "\n",
    "    feature_extractor = Model(inputs=vgg16.input, outputs=vgg16.layers[20].output)\n",
    "\n",
    "    image_reshape = [(cv2.resize(image, (224, 224))).reshape(1, 224, 224, 3)]\n",
    "    image_feature = feature_extractor(image_reshape)[0]\n",
    "    image_feature = np.ndarray.flatten(image_feature.numpy())\n",
    "    feature = np.concatenate((image_feature, history_feature))\n",
    "\n",
    "    return np.array([feature])\n",
    "\n",
    "# Q-Value computation\n",
    "def compute_q(feature, deep_q_model):\n",
    "    output = deep_q_model.predict(feature, verbose=0)\n",
    "    return np.ndarray.flatten(output)\n",
    "\n",
    "# Compute new mask based on the action\n",
    "def compute_mask(action, current_mask):\n",
    "    image_rate = 0.1\n",
    "    delta_width = image_rate * (current_mask[2] - current_mask[0])\n",
    "    delta_height = image_rate * (current_mask[3] - current_mask[1])\n",
    "\n",
    "    dx1, dy1, dx2, dy2 = 0, 0, 0, 0\n",
    "\n",
    "    # Apply the action to modify the bounding box (mask)\n",
    "    if action == 0:\n",
    "        dx1, dx2 = delta_width, delta_width\n",
    "    elif action == 1:\n",
    "        dx1, dx2 = -delta_width, -delta_width\n",
    "    elif action == 2:\n",
    "        dy1, dy2 = delta_height, delta_height\n",
    "    elif action == 3:\n",
    "        dy1, dy2 = -delta_height, -delta_height\n",
    "    elif action == 4:\n",
    "        dx1, dx2, dy1, dy2 = -delta_width, delta_width, -delta_height, delta_height\n",
    "    elif action == 5:\n",
    "        dx1, dx2, dy1, dy2 = delta_width, -delta_width, delta_height, -delta_height\n",
    "    elif action == 6:\n",
    "        dy1, dy2 = delta_height, -delta_height\n",
    "    elif action == 7:\n",
    "        dx1, dx2 = delta_width, -delta_width\n",
    "\n",
    "    new_mask_tmp = np.array([current_mask[0] + dx1, current_mask[1] + dy1,\n",
    "                             current_mask[2] + dx2, current_mask[3] + dy2])\n",
    "    new_mask = np.array([\n",
    "        min(new_mask_tmp[0], new_mask_tmp[2]),\n",
    "        min(new_mask_tmp[1], new_mask_tmp[3]),\n",
    "        max(new_mask_tmp[0], new_mask_tmp[2]),\n",
    "        max(new_mask_tmp[1], new_mask_tmp[3])\n",
    "    ])\n",
    "\n",
    "    return new_mask\n",
    "\n",
    "# Crop the image based on the mask\n",
    "def crop_image(image, new_mask):\n",
    "    height, width, channel = np.shape(image)\n",
    "    new_mask = np.asarray(new_mask).astype(\"int\")\n",
    "    new_mask[0] = max(new_mask[0], 0)\n",
    "    new_mask[1] = max(new_mask[1], 0)\n",
    "    new_mask[2] = min(new_mask[2], width)\n",
    "    new_mask[3] = min(new_mask[3], height)\n",
    "    cropped_image = image[new_mask[1]:new_mask[3], new_mask[0]:new_mask[2]]\n",
    "    new_height, new_width, new_channel = np.shape(cropped_image)\n",
    "\n",
    "    if new_height == 0 or new_width == 0:\n",
    "        cropped_image = np.zeros((224, 224, 3))\n",
    "    else:\n",
    "        cv2.resize(cropped_image, (224, 224))\n",
    "\n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c642ae-9b3a-4359-a95c-24a6d21734c4",
   "metadata": {},
   "source": [
    "# Load Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20b5087-78c2-4335-bd48-179fbdba5121",
   "metadata": {},
   "source": [
    "## Car Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1d616f30-d4ac-4719-b978-1c65bc179f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 282 images in test/images\n",
      "[[], [[68, 143, 604, 359]], [[5, 90, 625, 536]], [[514, 443, 528, 459], [530, 443, 546, 463], [541, 443, 558, 463], [566, 440, 583, 470], [600, 420, 640, 490], [586, 429, 610, 483], [143, 416, 219, 496], [0, 428, 26, 501], [41, 440, 105, 496]], []]\n",
      "DONE LOADING\n",
      "Found 282 images in test/images\n",
      "[[], [[68, 143, 604, 359]], [[5, 90, 625, 536]], [[514, 443, 528, 459], [530, 443, 546, 463], [541, 443, 558, 463], [566, 440, 583, 470], [600, 420, 640, 490], [586, 429, 610, 483], [143, 416, 219, 496], [0, 428, 26, 501], [41, 440, 105, 496]], []]\n",
      "DONE LOADING\n",
      "Only 152 images with one bounding box found.\n",
      "Selected 152 images and their corresponding bounding boxes.\n"
     ]
    }
   ],
   "source": [
    "test_image_list, test_bounding_box_list = load_data_fil(\"test\")\n",
    "test_image_list, test_bounding_box_list = load_data_fil(\"test\")\n",
    "\n",
    "# Ensure test_image_list and test_bounding_box_list have the same length\n",
    "assert len(test_image_list) == len(test_bounding_box_list), \"Image list and bounding box list must have the same length.\"\n",
    "\n",
    "# Find indices of images that have exactly one bounding box\n",
    "single_bbox_indices = [i for i, bbox_list in enumerate(test_bounding_box_list) if len(bbox_list) == 1]\n",
    "\n",
    "# Randomly select 20 such images (if there are at least 50)\n",
    "if len(single_bbox_indices) <200:\n",
    "    print(f\"Only {len(single_bbox_indices)} images with one bounding box found.\")\n",
    "    selected_indices = single_bbox_indices  # if less than 50, select all\n",
    "else:\n",
    "    selected_indices = random.sample(single_bbox_indices, 200)\n",
    "\n",
    "# Select the images and their corresponding bounding boxes\n",
    "selected_image_list = [test_image_list[i] for i in selected_indices]\n",
    "selected_bounding_box_list = [test_bounding_box_list[i] for i in selected_indices]\n",
    "\n",
    "print(f\"Selected {len(selected_image_list)} images and their corresponding bounding boxes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2084e17-09f4-4878-9fb5-3b0d5b3b0c2e",
   "metadata": {},
   "source": [
    "## Person Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "95f2e43c-6d63-4120-b137-2ec2ae9d8c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 282 images in test/images\n",
      "[[[136, 135, 270, 441], [283, 154, 469, 558]], [], [], [], [[273, 59, 449, 583], [85, 63, 255, 616]]]\n",
      "DONE LOADING\n",
      "Only 29 images with one bounding box found.\n",
      "Selected 29 images and their corresponding bounding boxes.\n"
     ]
    }
   ],
   "source": [
    "person_test_image_list, person_test_bounding_box_list = load_data_fil(\"test\", filtered_class=8)\n",
    "# Ensure test_image_list and test_bounding_box_list have the same length\n",
    "assert len(person_test_image_list) == len(person_test_bounding_box_list), \"Image list and bounding box list must have the same length.\"\n",
    "\n",
    "# Find indices of images that have exactly one bounding box\n",
    "single_bbox_indices = [i for i, bbox_list in enumerate(person_test_bounding_box_list) if len(bbox_list) == 1]\n",
    "\n",
    "# Randomly select 20 such images (if there are at least 50)\n",
    "if len(single_bbox_indices) <50:\n",
    "    print(f\"Only {len(single_bbox_indices)} images with one bounding box found.\")\n",
    "    selected_indices = single_bbox_indices  # if less than 50, select all\n",
    "else:\n",
    "    selected_indices = random.sample(single_bbox_indices, 50)\n",
    "\n",
    "# Select the images and their corresponding bounding boxes\n",
    "person_selected_image_list = [person_test_image_list[i] for i in selected_indices]\n",
    "person_selected_bounding_box_list = [person_test_bounding_box_list[i] for i in selected_indices]\n",
    "\n",
    "print(f\"Selected {len(person_selected_image_list)} images and their corresponding bounding boxes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fe1958-6441-481d-a19b-462f67c6aa81",
   "metadata": {},
   "source": [
    "# Load 2 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de2107d-d211-4cb5-858a-2401c2cc7301",
   "metadata": {},
   "source": [
    "## YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "65bcf401-5b89-4850-bb4d-06e9deeac600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /Users/khang/.cache/torch/hub/master.zip\n",
      "YOLOv5 ðŸš€ 2024-10-19 Python-3.8.13 torch-2.3.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7042489 parameters, 0 gradients, 15.9 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path='yolov5/runs/train/yolov5_custom_augmented3/weights/best.pt', force_reload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e87b1f-1693-4905-a04b-64b00e44b4ec",
   "metadata": {},
   "source": [
    "## Deep QN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17b19fa-50ba-427c-8529-7e559a37be92",
   "metadata": {},
   "source": [
    "### Car Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "64fcaaba-02a1-4de1-9d50-b2e1d48469a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_size = 10\n",
    "action_option = 9\n",
    "\n",
    "# Change the model name with different parameters\n",
    "model_link = 'well_train_model_max_step_20_gamma_0.1_epochs_10_trigger_threshold_0.5.h5'\n",
    "vgg16 = create_vgg16()\n",
    "deep_q_car = create_q_model()\n",
    "deep_q_car.load_weights(model_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7ce2b4-0ba9-47cc-ab8a-51a9d8d58acf",
   "metadata": {},
   "source": [
    "### Person Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2045009b-fd6a-4681-b7a3-b7bb64229d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_size = 10\n",
    "action_option = 9\n",
    "\n",
    "person_model_link = 'person_well_train_model_max_step_20_gamma_0.1_epochs_10_trigger_threshold_0.5.h5'\n",
    "vgg16 = create_vgg16()\n",
    "deep_q_person = create_q_model()\n",
    "deep_q_person.load_weights(person_model_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06717309-c5db-49b8-b462-ebe3cf91197b",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bb031a-46b1-4b93-9de2-f5f6fce4f84a",
   "metadata": {},
   "source": [
    "## Car "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5acf60-3df2-4ce6-bb2b-baae66e72092",
   "metadata": {},
   "source": [
    "### YOLOv5 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1ab1fb8-aa70-4a47-9f17-e98d31b45926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_yolo(selected_image_list, selected_bounding_box_list):\n",
    "    iou_list = []\n",
    "    for i in range(0,len(selected_image_list)):\n",
    "        plt.figure()\n",
    "        image = selected_image_list[i]\n",
    "        bounding_box = selected_bounding_box_list[i][0]  # Assuming this is the ground truth\n",
    "    \n",
    "        # Perform prediction using YOLOv5\n",
    "        results = model(image)\n",
    "        predictions = results.xyxy[0]  # Predictions in (x1, y1, x2, y2, conf, class)\n",
    "        print(len(predictions))\n",
    "        if len(predictions) > 0:\n",
    "            pred_box = predictions[0][:4].cpu().numpy()  # Taking the first predicted bounding box\n",
    "            pred_box = pred_box.astype(int)\n",
    "    \n",
    "            # Draw predicted bounding box (in blue)\n",
    "            cv2.rectangle(image, (pred_box[0], pred_box[1]), (pred_box[2], pred_box[3]), (0, 0, 255), 2)\n",
    "    \n",
    "        # Draw ground truth bounding box (in green)\n",
    "        cv2.rectangle(image, (int(bounding_box[0]), int(bounding_box[1])),\n",
    "                      (int(bounding_box[2]), int(bounding_box[3])), (0, 255, 0), 2)\n",
    "    \n",
    "        # Calculate IoU if a prediction exists\n",
    "        if len(predictions) > 0:\n",
    "            iou = compute_iou(pred_box, bounding_box)\n",
    "            iou_list.append(iou)\n",
    "        else:\n",
    "            iou_list.append(0)            \n",
    "    \n",
    "        # Plot results\n",
    "        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"Predicted box in blue, Ground truth in green (Image {i})\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    # Print average IoU\n",
    "    print(f\"Average IoU: {sum(iou_list) / len(iou_list)}\")\n",
    "    print(\"Length of IOU =\", len(iou_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa9d286-0e59-43ef-888f-6fbc55ddc312",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_yolo(selected_image_list, selected_bounding_box_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a620a6-b190-4cb1-b581-e55c1e233ae6",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "694df7a1-2dad-4512-bd57-70f433697608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_agent(image, bounding_box):\n",
    "    print(\"PREDICTING WITH AGENT\")\n",
    "    history = [-1] * history_size\n",
    "    height, width, channel = np.shape(image)\n",
    "    \n",
    "    current_mask = np.asarray([0, 0, width, height])\n",
    "    \n",
    "    feature = extract_feature(image, history, vgg16)\n",
    "    end = False\n",
    "    masks = []\n",
    "    step = 0\n",
    "\n",
    "    while not end:\n",
    "\n",
    "        q_value = compute_q(feature, deep_q)\n",
    "\n",
    "        action = np.argmax(q_value)\n",
    "        print(\"ACTION = \",action)\n",
    "\n",
    "        history = history[1:]\n",
    "        history.append(action)\n",
    "\n",
    "        if action == 8 or step == 10:\n",
    "            end = True\n",
    "            new_mask = current_mask\n",
    "            return new_mask, step\n",
    "        else:\n",
    "            new_mask = compute_mask(action, current_mask)\n",
    "\n",
    "        cropped_image = crop_image(image, new_mask)\n",
    "        feature = extract_feature(cropped_image, history, vgg16)\n",
    "\n",
    "        masks.append(new_mask)\n",
    "        current_mask = new_mask\n",
    "        cv2.rectangle(image, (int(current_mask[0]), int(current_mask[1])),\n",
    "                      (int(current_mask[2]), int(current_mask[3])), (255, 0, 0), 1)\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f18a65c8-cd07-4fcb-8ec2-d8eac6813323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "def test_agent(selected_image_list, selected_bounding_box_list, num_imgs = 152):\n",
    "    iou_list = []\n",
    "    iou_list_yolo = []\n",
    "    num_yolo_work = 0\n",
    "    num_dqn_work = 0\n",
    "    num_dqn_accepted = 0\n",
    "\n",
    "    num_steps = []\n",
    "    \n",
    "    for i in range(num_imgs):\n",
    "        \n",
    "        image = selected_image_list[i]\n",
    "        bounding_box = selected_bounding_box_list[i][0]  # Assuming this is the ground truth\n",
    "    \n",
    "        num_dqn_work += 1\n",
    "        pred_box, step = predict_with_agent(image, bounding_box)\n",
    "        \n",
    "        num_steps.append(step)\n",
    "        \n",
    "        iou = compute_iou(pred_box.astype(int), bounding_box)\n",
    "        if iou >= 0.5:\n",
    "            num_dqn_accepted += 1\n",
    "\n",
    "        iou_list.append(iou)\n",
    "        # Draw ground truth bounding box (in green)\n",
    "        cv2.rectangle(image, (int(bounding_box[0]), int(bounding_box[1])),\n",
    "                      (int(bounding_box[2]), int(bounding_box[3])), (0, 255, 0), 2)\n",
    "\n",
    "        cv2.rectangle(image, (int(pred_box[0]), int(pred_box[1])),\n",
    "                          (int(pred_box[2]), int(pred_box[3])), (0, 0, 255), 2)\n",
    "        \n",
    "        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "           \n",
    "                \n",
    "    \n",
    "    # Print average IoU\n",
    "    print(f\"Average IoU: {sum(iou_list) / len(iou_list)}\")\n",
    "    print(f\"Average IoU: {sum(iou_list_yolo) / len(iou_list_yolo)}\")\n",
    "    \n",
    "    #print(\"Number of times agent making prediction\", num_dqn_work)\n",
    "    #print(\"Number of times yolo making prediction\", num_dqn_work)\n",
    "    #print(\"Number of times agent making prediction and be accepted\", num_dqn_accepted)\n",
    "\n",
    "    return num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316a4993-4ad9-4f33-92b4-14a957a7fcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_q = deep_q_car\n",
    "num_steps = test_agent(selected_image_list, selected_bounding_box_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb0fc73-6f93-4b11-985b-6d94781bbc7f",
   "metadata": {},
   "source": [
    "### YOLO + DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0923a242-b288-4967-a4d7-c20e3c28c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "def test_yolo_agent(selected_image_list, selected_bounding_box_list, num_imgs = 152):\n",
    "    iou_list = []\n",
    "    iou_list_yolo = []\n",
    "    num_yolo_work = 0\n",
    "    num_dqn_work = 0\n",
    "    num_dqn_accepted = 0\n",
    "\n",
    "    num_steps = []\n",
    "    \n",
    "    for i in range(num_imgs):\n",
    "        \n",
    "        image = selected_image_list[i]\n",
    "        bounding_box = selected_bounding_box_list[i][0]  # Assuming this is the ground truth\n",
    "    \n",
    "        Perform prediction using YOLOv5\n",
    "        results = model(image)\n",
    "        predictions = results.xyxy[0]  # Predictions in (x1, y1, x2, y2, conf, class)\n",
    "        print(len(predictions))\n",
    "        if len(predictions) > 0:\n",
    "            pred_box = predictions[0][:4].cpu().numpy()  # Taking the first predicted bounding box\n",
    "            pred_box = pred_box.astype(int)\n",
    "\n",
    "             # Calculate IoU if a prediction exists\n",
    "            if len(predictions) > 0:\n",
    "                iou = compute_iou(pred_box.astype(int), bounding_box)\n",
    "                iou_list.append(iou)\n",
    "                iou_list_yolo.append(iou)\n",
    "                num_yolo_work += 1\n",
    "    \n",
    "        else:\n",
    "            num_dqn_work += 1\n",
    "            pred_box, step = predict_with_agent(image, bounding_box)\n",
    "            \n",
    "            num_steps.append(step)\n",
    "            \n",
    "            iou = compute_iou(pred_box.astype(int), bounding_box)\n",
    "            if iou >= 0.5:\n",
    "                num_dqn_accepted += 1\n",
    "                \n",
    "            iou_list.append(iou)\n",
    "            # Draw ground truth bounding box (in green)\n",
    "            cv2.rectangle(image, (int(bounding_box[0]), int(bounding_box[1])),\n",
    "                          (int(bounding_box[2]), int(bounding_box[3])), (0, 255, 0), 2)\n",
    "    \n",
    "            cv2.rectangle(image, (int(pred_box[0]), int(pred_box[1])),\n",
    "                              (int(pred_box[2]), int(pred_box[3])), (0, 0, 255), 2)\n",
    "            \n",
    "            plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "           \n",
    "                \n",
    "    \n",
    "    #Print average IoU\n",
    "    print(f\"Average IoU: {sum(iou_list) / len(iou_list)}\")\n",
    "    print(f\"Average IoU: {sum(iou_list_yolo) / len(iou_list_yolo)}\")\n",
    "    \n",
    "    #print(\"Number of times agent making prediction\", num_dqn_work)\n",
    "    #print(\"Number of times yolo making prediction\", num_dqn_work)\n",
    "    #print(\"Number of times agent making prediction and be accepted\", num_dqn_accepted)\n",
    "\n",
    "    return num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d55006-92b2-4165-b735-96bfb2204640",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_yolo_agent(selected_image_list, selected_bounding_box_list, num_imgs = 152)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989a6b45-c5ee-461a-8d66-f7f8f95d0896",
   "metadata": {},
   "source": [
    "## Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdf3516-5030-4cf5-a994-a09a6852c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_yolo(person_selected_image_list, person_selected_bounding_box_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661d00bc-4685-4b29-b05a-c4d67d8662be",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_q = deep_q_person\n",
    "num_steps = test_agent(person_selected_image_list, person_selected_bounding_box_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcab0d62-aa9c-4314-8aac-7c5f6666c77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_yolo_agent(person_selected_image_list, person_selected_bounding_box_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4371504d-ec86-4773-af66-2d7fa33aebef",
   "metadata": {},
   "source": [
    "## Number of Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6c7851-070c-45ae-87dd-059ff72ede98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each unique number\n",
    "unique, counts = np.unique(num_steps, return_counts=True)\n",
    "\n",
    "# Create the bar plot\n",
    "plt.bar(unique, counts)\n",
    "plt.xlabel('Number of regions')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Number of regions analysed per object')\n",
    "plt.xticks(unique)  # Set x-ticks to the unique values\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4e6a49-7882-480c-afaa-f9103523c5b4",
   "metadata": {},
   "source": [
    "# Precision - Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01cf140-b7f2-4f7f-92da-78fbd40d2345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def evaluate_precision_recall(model_link, selected_image_list, selected_bounding_box_list, num_imgs=100, target_size=(224, 224)):\n",
    "    iou_list = []\n",
    "    ground_truths = []\n",
    "    reward_scores = []\n",
    "\n",
    "    vgg16 = create_vgg16()\n",
    "    deep_q = create_q_model()\n",
    "    deep_q.load_weights(model_link)\n",
    "\n",
    "    for i in range(0, num_imgs):\n",
    "        bounding_box = selected_bounding_box_list[i][0]  # Ground truth bounding box\n",
    "        image = selected_image_list[i]  # Original image\n",
    "\n",
    "        predicted_box = None\n",
    "\n",
    "        # Perform prediction using YOLOv5\n",
    "        results = model(image)\n",
    "        predictions = results.xyxy[0]  # Predictions in (x1, y1, x2, y2, conf, class)\n",
    "        print(len(predictions))\n",
    "        if len(predictions) > 0:\n",
    "            pred_box = predictions[0][:4].cpu().numpy()  # Taking the first predicted bounding box\n",
    "            pred_box = pred_box.astype(int)\n",
    "        else:\n",
    "            history = [-1] * history_size\n",
    "            height, width, channel = np.shape(image)\n",
    "    \n",
    "            current_mask = np.asarray([0, 0, width, height])  # Initial full image mask\n",
    "            feature = extract_feature(image, history, vgg16)\n",
    "            end = False\n",
    "            masks = []\n",
    "            step = 0\n",
    "            predicted_box = None\n",
    "            cropped_image = image.copy()\n",
    "    \n",
    "            while not end:\n",
    "                q_value = compute_q(feature, deep_q)\n",
    "                action = np.argmax(q_value)\n",
    "    \n",
    "                history = history[1:]\n",
    "                history.append(action)\n",
    "    \n",
    "                if action == 8 or step == 7:\n",
    "                    end = True\n",
    "                    predicted_box = current_mask  # The final predicted bounding box\n",
    "                    reward_scores.append(q_value[5])  # Assuming the sixth neuron corresponds to terminal action\n",
    "                    break\n",
    "                else:\n",
    "                    new_mask = compute_mask(action, current_mask)\n",
    "                    cropped_image = crop_image(image, new_mask)\n",
    "                    feature = extract_feature(cropped_image, history, vgg16)\n",
    "                    current_mask = new_mask\n",
    "    \n",
    "                step += 1\n",
    "\n",
    "        if predicted_box is not None:\n",
    "            # Compute IoU between the predicted bounding box and the ground truth\n",
    "            iou = compute_iou(predicted_box, bounding_box)\n",
    "            iou_list.append(iou)\n",
    "            \n",
    "            # Classify as True Positive (TP) or False Positive (FP) based on IoU threshold of 0.5\n",
    "            if iou >= 0.5:\n",
    "                ground_truths.append(1)  # TP\n",
    "            else:\n",
    "                ground_truths.append(0)  # FP\n",
    "\n",
    "    # Compute Precision and Recall using ground_truths and reward_scores\n",
    "    precision, recall, _ = precision_recall_curve(ground_truths, reward_scores)\n",
    "\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def plot_precision_recall_curve(precision, recall, model_name):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, label=f'{model_name} Model')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall Curve: {model_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "test_image_list, test_bounding_box_list = load_data_fil(\"valid\")\n",
    "\n",
    "# Ensure test_image_list and test_bounding_box_list have the same length\n",
    "assert len(test_image_list) == len(test_bounding_box_list), \"Image list and bounding box list must have the same length.\"\n",
    "\n",
    "# Find indices of images that have exactly one bounding box\n",
    "single_bbox_indices = [i for i, bbox_list in enumerate(test_bounding_box_list) if len(bbox_list) == 1]\n",
    "\n",
    "# Randomly select 20 such images (if there are at least 50)\n",
    "if len(single_bbox_indices) <200:\n",
    "    print(f\"Only {len(single_bbox_indices)} images with one bounding box found.\")\n",
    "    selected_indices = single_bbox_indices  # if less than 50, select all\n",
    "else:\n",
    "    selected_indices = random.sample(single_bbox_indices, 200)\n",
    "\n",
    "# Select the images and their corresponding bounding boxes\n",
    "selected_image_list = [test_image_list[i] for i in selected_indices]\n",
    "selected_bounding_box_list = [test_bounding_box_list[i] for i in selected_indices]\n",
    "\n",
    "print(f\"Selected {len(selected_image_list)} images and their corresponding bounding boxes.\")\n",
    "\n",
    "# Example of calling the function with your model and images\n",
    "precision, recall = evaluate_precision_recall(model_link, selected_image_list, selected_bounding_box_list)\n",
    "plot_precision_recall_curve(precision, recall, \"DQN Agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6166724-0a5f-4ea9-8fa8-8971ec0e40a0",
   "metadata": {},
   "source": [
    "# Searching for Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4f26c01c-4369-44d4-816b-5ed3324b757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def searching_objects_clear_boxes(model_link, selected_image_list, selected_bounding_box_list, num_imgs=100, target_size=(224, 224)):\n",
    "    object_number = 1\n",
    "    iou = []\n",
    "    vgg16 = create_vgg16()\n",
    "    deep_q = create_q_model()\n",
    "    deep_q.load_weights(model_link)\n",
    "\n",
    "    for i in range(0, num_imgs):\n",
    "        bounding_box = selected_bounding_box_list[i][0]  # Ground truth bounding box\n",
    "        image = selected_image_list[i]  # Original image\n",
    "        history = [-1] * history_size\n",
    "        height, width, channel = np.shape(image)\n",
    "        \n",
    "        current_mask = np.asarray([0, 0, width, height])  # Initial full image mask\n",
    "        \n",
    "        feature = extract_feature(image, history, vgg16)\n",
    "        end = False\n",
    "        masks = []\n",
    "        cropped_images = []  # To store the previous cropped images for each step\n",
    "        step = 0\n",
    "\n",
    "        # Initial cropped image (whole image at the beginning)\n",
    "        cropped_image = image.copy()\n",
    "\n",
    "        while not end:\n",
    "\n",
    "            q_value = compute_q(feature, deep_q)\n",
    "            action = np.argmax(q_value)\n",
    "\n",
    "            history = history[1:]\n",
    "            history.append(action)\n",
    "\n",
    "            # Draw bounding boxes on a fresh copy of the cropped image for each step\n",
    "            step_image = cropped_image.copy()\n",
    "\n",
    "            if action == 8 or step == 7:\n",
    "                end = True\n",
    "                new_mask = current_mask\n",
    "\n",
    "                # Draw the final predicted bounding box (in blue) on the final cropped image\n",
    "                cv2.rectangle(step_image, (int(new_mask[0]), int(new_mask[1])),\n",
    "                              (int(new_mask[2]), int(new_mask[3])), (0, 0, 255), 2)\n",
    "            \n",
    "                # Draw the ground truth bounding box (in green)\n",
    "                #cv2.rectangle(step_image, (int(bounding_box[0]), int(bounding_box[1])),\n",
    "                #              (int(bounding_box[2]), int(bounding_box[3])), (0, 255, 0), 2)\n",
    "                    \n",
    "                cropped_images.append(cv2.resize(step_image.copy(), target_size))  # Resize before storing\n",
    "                masks.append(new_mask)\n",
    "                break\n",
    "            else:\n",
    "                # Compute new mask based on action\n",
    "                new_mask = compute_mask(action, current_mask)\n",
    "\n",
    "                # Draw the current predicted bounding box (in red) on the fresh cropped image for this step\n",
    "                cv2.rectangle(step_image, (int(new_mask[0]), int(new_mask[1])),\n",
    "                              (int(new_mask[2]), int(new_mask[3])), (255, 0, 0), 2)\n",
    "\n",
    "                # Resize the fresh image for plotting, without affecting the next step\n",
    "                cropped_images.append(step_image.copy())\n",
    "\n",
    "                # Crop the image for the next step\n",
    "                cropped_image = crop_image(image, new_mask)\n",
    "\n",
    "                feature = extract_feature(cropped_image, history, vgg16)\n",
    "                masks.append(new_mask)\n",
    "                current_mask = new_mask\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        # Plot the cropped images, each with only the bounding box for its respective step\n",
    "        n_steps = len(cropped_images) - 1\n",
    "        if n_steps <= 1:\n",
    "            continue\n",
    "        # Compute IoU for the final mask\n",
    "        mask = masks[-1]\n",
    "        iou_ = compute_iou(mask, bounding_box)\n",
    "        iou.append(iou_)\n",
    "\n",
    "        if iou_ >= 0.2:\n",
    "            fig, axes = plt.subplots(1, n_steps, figsize=(n_steps * 2, 5))  # Adjust figure size\n",
    "            for j in range(n_steps):\n",
    "                axes[j].imshow(cv2.cvtColor(cropped_images[j], cv2.COLOR_BGR2RGB))  # Convert BGR to RGB\n",
    "                axes[j].axis('off')  # Hide axis\n",
    "                #axes[j].set_title(f'Step {j + 1}')  # Title with step number\n",
    "    \n",
    "            plt.suptitle(f\"Search path for Image {i} - Red: Predicted box, Green: Ground truth\")\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3da68a-e623-403d-8b83-5e1d942413fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "searching_objects_clear_boxes(model_link, selected_image_list, \\\n",
    "     selected_bounding_box_list, num_imgs = 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
